= Fluent-bit logging in Kubernetes

:title: Fluent-bit logging in Kubernetes
:date: 2020-05-22
:tags: kubernetes, observability, cloud-native, fluent-bit, fluentd, elasticsearch
:slug: fluent-bit-logging
:authors: George Goh
:summary: Logging in Kubernetes
:status: draft

Fluent Bit is a fast and lightweight log processor, stream processor and 
forwarder. It's gained popularity as the younger sibling of Fluentd due
to its tiny memory footprint(~450KB compared to Fluentd's ~40MB), and
zero dependencies - making it ideal for cloud and edge computing use cases.

In this post, I'll share my research, issues and workarounds in getting a
lab set up for logging in a single Kubernetes cluster.

== High-level overview

First, let's look at the big picture:

image::/images/fluent-bit-big-picture.png[Logging Pipeline Big Picture,100%]

From the left, we see a single Kubernetes cluster which has a fluent-bit pod 
running on each node as part of a Daemonset, picking up logs from 
`/var/log/containers/*.log` on each node. The fluent-bit pods forward these logs
to an external Fluentd node.

The external Fluentd node aggregates logs from the all the fluent-bit instances.
It's configured to post-process and create additional indexes to separate
application logs(kube.\*) from system logs(kube-ops.*), which are then sent
on to an Elasticsearch instance.

Elasticsearch and Kibana sit on the same node and the `Log Viewer` user accesses
logs through the Kibana web UI. A `Elasticsearch Ops` user can use a tool like
cerebro to view the status of Elasticsearch(node health, indexes, storage, and
so on).

In the following sections, we'll be building up the architecture in the above
diagram, going from left to right.

=== Fluent Bit

Fluent Bit has a well-documented link:https://docs.fluentbit.io/manual/concepts/data-pipeline[data pipeline]. The plugins used in each stage of this pipeline are
as shown below:

image::/images/fluent-bit-pipeline.png[fluent-bit Logging Pipeline,100%]

We'll use link:https://helm.sh[helm] to deploy this.

==== Prerequisites
* helm3 - https://helm.sh

==== Deploy Fluent Bit

. Save the following in a file called `values.yaml`.
```
on_minikube: false

image:
  fluent_bit:
    repository: fluent/fluent-bit
    tag: v1.3.7
  pullPolicy: Always

# When enabled, exposes json and prometheus metrics on {{ .Release.Name }}-metrics service
metrics:
  enabled: true
  service:
    labels:
       k8s-app: fluent-bit
    annotations:
      'prometheus.io/path': "/api/v1/metrics/prometheus"
      'prometheus.io/port': "2020"
      'prometheus.io/scrape': "true"
    port: 2020
    type: ClusterIP
  serviceMonitor:
    enabled: false
    additionalLabels: {}
    # namespace: monitoring
    # interval: 30s
    # scrapeTimeout: 10s

backend:
  type: forward
  forward:
    host: fluentd-0.lab.spodon.com
    port: 24224
    tls: "off"
    tls_verify: "on"
    tls_debug: 1
    shared_key: thisisunsafe

parsers:
  enabled: true
  ## List the respective parsers in key: value format per entry
  ## Regex required fields are name and regex. JSON and Logfmt required field
  ## is name.
  regex:
    - name: cri-mod
      regex: "^(?<time>[^ ]+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<log>.*)$"
      timeFormat: "%Y-%m-%dT%H:%M:%S.%L%z"
      timeKey: time
    - name: catchall
      regex: "^(?<message>.*)$ }"
  logfmt: []
  json: []

env: []
podAnnotations: {}
fullConfigMap: false
existingConfigMap: ""
rawConfig: |-
  @INCLUDE fluent-bit-service.conf
  @INCLUDE fluent-bit-input.conf
  @INCLUDE fluent-bit-filter.conf
  @INCLUDE fluent-bit-output.conf

extraEntries:
  input: ""
  audit: ""
  filter: |
    Merge_Parser     catchall
    Keep_Log         Off
  output: ""

extraPorts: []

extraVolumes: []

extraVolumeMounts: []

resources: {}
hostNetwork: false
dnsPolicy: ClusterFirst
tolerations: []
nodeSelector: {}
affinity: {}
service:
  flush: 1
  logLevel: info

input:
  tail:
    memBufLimit: 5MB
    parser: cri-mod
    path: /var/log/containers/*.log
    ignore_older: ""
  systemd:
    enabled: false
    filters:
      systemdUnit:
        - docker.service
        - kubelet.service
        - node-problem-detector.service
    maxEntries: 1000
    readFromTail: true
    stripUnderscores: false
    tag: host.*

audit:
  enable: false
  input:
    memBufLimit: 35MB
    parser: docker
    tag: audit.*
    path: /var/log/kube-apiserver-audit.log
    bufferChunkSize: 2MB
    bufferMaxSize: 10MB
    skipLongLines: true
    key: kubernetes-audit

filter:
  kubeURL: https://kubernetes.default.svc:443
  kubeCAFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  kubeTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
  kubeTag: kube
  kubeTagPrefix: kube.var.log.containers.
  mergeJSONLog: true
  mergeLogKey: ""
  enableParser: true
  enableExclude: true
  useJournal: false

rbac:
  create: true
  pspEnabled: false

taildb:
  directory: /var/lib/fluent-bit

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

## Specifies security settings for a container
## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
securityContext: {}
  # securityContext:
  #   privileged: true

## Specifies security settings for a pod
## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
podSecurityContext: {}
  # podSecurityContext:
  #   runAsUser: 1000
```

[start=2]
. Install the Fluent Bit helm chart, using values created in the previous step.
```
helm install --name fluent-bit -f values.yaml stable/fluent-bit
```

. Fluent Bit setup is done here. Next we set up Fluentd. 